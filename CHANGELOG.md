# Changelog

Táº¥t cáº£ cÃ¡c thay Ä‘á»•i quan trá»ng cá»§a dá»± Ã¡n Ä‘Æ°á»£c ghi láº¡i táº¡i Ä‘Ã¢y.

## [0.1.0] - 2025-11-01

### âœ¨ Added - TÃ­nh nÄƒng má»›i

**Backend (FastAPI):**
- API endpoint `/health` Ä‘á»ƒ health check
- API endpoint `/predict` Ä‘á»ƒ nháº­n diá»‡n cáº£m xÃºc tá»« audio
- Há»— trá»£ multipart form upload (audio file + optional transcript)
- TÃ­ch há»£p Wav2Vec2 model (`superb/wav2vec2-base-superb-er`) cho audio emotion
- TÃ­ch há»£p DistilRoBERTa model (`j-hartmann/emotion-english-distilroberta-base`) cho text emotion
- CÆ¡ cháº¿ fusion Ä‘a phÆ°Æ¡ng thá»©c (audio + text) vá»›i trá»ng sá»‘ Ä‘iá»u chá»‰nh
- CORS middleware Ä‘á»ƒ support web frontend
- Pydantic schemas cho type-safe responses
- Audio processing utilities: load audio, MFCC, Mel-spectrogram
- Label mapping vá» 7 base emotions (anger, disgust, fear, joy, sadness, surprise, neutral)

**Web Frontend:**
- Upload audio files (.wav, .mp3, etc.)
- Audio player preview
- Optional transcript input (English)
- Adjustable audio weight slider (0-1)
- Display results: final label, score, top-k from 3 sources
- Dark theme responsive design
- Error handling vá»›i user-friendly messages
- Auto-scroll to results
- File info display (name, size)

**Documentation:**
- README.md vá»›i giá»›i thiá»‡u Ä‘áº§y Ä‘á»§
- QUICKSTART.md vá»›i hÆ°á»›ng dáº«n cháº¡y chi tiáº¿t
- COMMANDS.md vá»›i quick reference
- backend/README.md vá»›i API docs
- web/README.md vá»›i frontend usage
- Inline code comments

**Scripts & Tools:**
- `run_backend.ps1` - Script PowerShell tá»± Ä‘á»™ng cháº¡y backend
- `check_backend.py` - Health check script
- `create_sample_audio.py` - Táº¡o file audio test
- `.gitignore` - Ignore Python/IDE/cache files
- `requirements.txt` - Python dependencies

**Tests:**
- `test_audio_processing.py` - Unit test cho MFCC computation

### ğŸ—ï¸ Architecture

**Cáº¥u trÃºc dá»± Ã¡n:**
```
NCKH2025/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py                      # FastAPI app
â”‚   â”‚   â”œâ”€â”€ schemas.py                   # Pydantic models
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â””â”€â”€ emotion_inference.py     # Wav2Vec2 pipeline
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ audio_processing.py      # Audio utils
â”‚   â”‚   â”‚   â”œâ”€â”€ text_inference.py        # BERT pipeline
â”‚   â”‚   â”‚   â””â”€â”€ fusion.py                # Multi-modal fusion
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â””â”€â”€ test_audio_processing.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ web/
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ styles.css
â”‚   â”œâ”€â”€ script.js
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ run_backend.ps1
â”œâ”€â”€ check_backend.py
â”œâ”€â”€ create_sample_audio.py
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ QUICKSTART.md
â”œâ”€â”€ COMMANDS.md
â””â”€â”€ CHANGELOG.md
```

**Tech Stack:**
- Python 3.12+
- FastAPI 0.115+
- Transformers 4.44+ (Hugging Face)
- PyTorch 2.3+
- Librosa 0.10+ (audio processing)
- Uvicorn (ASGI server)
- Vanilla JS + HTML5 + CSS3 (frontend)

### ğŸ“Š Models

**Audio Emotion Recognition:**
- Model: `superb/wav2vec2-base-superb-er`
- Type: Wav2Vec2 (Transformer-based)
- Task: Emotion Recognition from speech
- Source: SUPERB benchmark

**Text Emotion Classification:**
- Model: `j-hartmann/emotion-english-distilroberta-base`
- Type: DistilRoBERTa (BERT-family)
- Task: Emotion classification from text
- Language: English

### ğŸ¯ Emotions Supported

7 base emotions:
1. Anger (tá»©c giáº­n)
2. Disgust (ghÃª tá»Ÿm)
3. Fear (sá»£ hÃ£i)
4. Joy (vui váº»)
5. Sadness (buá»“n bÃ£)
6. Surprise (ngáº¡c nhiÃªn)
7. Neutral (trung tÃ­nh)

### ğŸ“ Notes

- MVP version - proof of concept
- Models Ä‘Æ°á»£c cache local sau láº§n táº£i Ä‘áº§u tiÃªn
- Inference speed: 1-3s cho audio < 10s (CPU)
- Default fusion weights: audio 0.7, text 0.3
- Target sampling rate: 16kHz
- CORS enabled cho localhost development

### ğŸš§ Known Limitations

- Chá»‰ há»— trá»£ tiáº¿ng Anh (models trained on English)
- ChÆ°a cÃ³ ASR tá»± Ä‘á»™ng (transcript nháº­p tay)
- ChÆ°a tá»‘i Æ°u cho production (single worker)
- ChÆ°a cÃ³ logging/monitoring
- ChÆ°a cÃ³ authentication
- ChÆ°a test trÃªn datasets chuáº©n (RAVDESS, EMO-DB)
- Frontend Ä‘Æ¡n giáº£n (no real-time, no batch)

---

## [Unreleased] - Roadmap

### ğŸ”® Planned Features

**Phase 2: Data & Evaluation**
- [ ] TÃ­ch há»£p RAVDESS dataset
- [ ] TÃ­ch há»£p EMO-DB dataset
- [ ] Module Ä‘Ã¡nh giÃ¡: Accuracy, F1, Precision, Recall
- [ ] Confusion Matrix visualization
- [ ] Benchmark vá»›i baseline methods

**Phase 3: ASR Integration**
- [ ] TÃ­ch há»£p Whisper cho speech-to-text
- [ ] Hoáº·c Vosk (offline ASR)
- [ ] Auto transcript generation

**Phase 4: Model Improvement**
- [ ] Fine-tune trÃªn classroom domain
- [ ] Noise reduction pipeline
- [ ] Domain adaptation
- [ ] Multi-lingual support (Vietnamese)

**Phase 5: Production Ready**
- [ ] Docker containerization
- [ ] CI/CD pipeline
- [ ] Logging (structlog)
- [ ] Monitoring (Prometheus)
- [ ] Authentication & Authorization
- [ ] Rate limiting
- [ ] Caching layer (Redis)

**Phase 6: Advanced Frontend**
- [ ] Real-time emotion tracking
- [ ] Dashboard analytics cho giáº£ng viÃªn
- [ ] Batch upload
- [ ] Waveform visualization
- [ ] Export reports (PDF, CSV)

**Phase 7: Research**
- [ ] Publish results
- [ ] Comparison study
- [ ] User evaluation (teachers, students)
- [ ] Qualitative analysis

---

**Format:** [version] - date  
**Types:** Added, Changed, Deprecated, Removed, Fixed, Security

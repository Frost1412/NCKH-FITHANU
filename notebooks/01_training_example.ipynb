{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "100c98a9",
   "metadata": {},
   "source": [
    "# Emotion Recognition Training Example\n",
    "\n",
    "This notebook demonstrates how to train an emotion recognition model using the RAVDESS dataset.\n",
    "\n",
    "## Table of Contents\n",
    "1. Setup and Installation\n",
    "2. Data Exploration\n",
    "3. Data Preprocessing\n",
    "4. Model Training\n",
    "5. Evaluation\n",
    "6. Inference on New Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acb4b2d",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ae7f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets torch torchaudio librosa pandas scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aef281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import torch\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification\n",
    "\n",
    "# Set style\n",
    "sns.set_style('darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Check GPU availability\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943ca46f",
   "metadata": {},
   "source": [
    "## 2. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4bad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset statistics\n",
    "import json\n",
    "\n",
    "stats_path = Path('../data/processed/ravdess_statistics.json')\n",
    "if stats_path.exists():\n",
    "    with open(stats_path) as f:\n",
    "        stats = json.load(f)\n",
    "    \n",
    "    print(\"Dataset Statistics:\")\n",
    "    print(json.dumps(stats, indent=2))\n",
    "else:\n",
    "    print(\"Statistics file not found. Please run prepare_ravdess.py first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8ed10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "train_df = pd.read_csv('../data/processed/ravdess_train.csv')\n",
    "val_df = pd.read_csv('../data/processed/ravdess_val.csv')\n",
    "test_df = pd.read_csv('../data/processed/ravdess_test.csv')\n",
    "\n",
    "print(f\"Train samples: {len(train_df)}\")\n",
    "print(f\"Val samples: {len(val_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "\n",
    "# Display first few rows\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999d2839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize emotion distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for ax, (name, df) in zip(axes, [(\"Train\", train_df), (\"Val\", val_df), (\"Test\", test_df)]):\n",
    "    emotion_counts = df['label'].value_counts().sort_index()\n",
    "    ax.bar(emotion_counts.index, emotion_counts.values)\n",
    "    ax.set_title(f\"{name} Set Emotion Distribution\")\n",
    "    ax.set_xlabel(\"Emotion\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bd553f",
   "metadata": {},
   "source": [
    "### Audio Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20a194f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and visualize sample audio for each emotion\n",
    "emotions = train_df['label'].unique()\n",
    "\n",
    "fig, axes = plt.subplots(len(emotions), 2, figsize=(15, 3*len(emotions)))\n",
    "\n",
    "for i, emotion in enumerate(sorted(emotions)):\n",
    "    # Get first sample for this emotion\n",
    "    sample_path = train_df[train_df['label'] == emotion].iloc[0]['file_path']\n",
    "    \n",
    "    # Load audio\n",
    "    y, sr = librosa.load(sample_path, sr=16000)\n",
    "    \n",
    "    # Plot waveform\n",
    "    librosa.display.waveshow(y, sr=sr, ax=axes[i, 0])\n",
    "    axes[i, 0].set_title(f\"{emotion.upper()} - Waveform\")\n",
    "    axes[i, 0].set_xlabel(\"Time (s)\")\n",
    "    \n",
    "    # Plot mel spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    librosa.display.specshow(mel_spec_db, sr=sr, x_axis='time', y_axis='mel', ax=axes[i, 1])\n",
    "    axes[i, 1].set_title(f\"{emotion.upper()} - Mel Spectrogram\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da50d556",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40426d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path to import training utilities\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "from training.utils.audio_preprocessing import load_and_preprocess_audio, extract_features\n",
    "\n",
    "# Test preprocessing\n",
    "sample_path = train_df.iloc[0]['file_path']\n",
    "y, sr = load_and_preprocess_audio(sample_path, target_sr=16000)\n",
    "\n",
    "print(f\"Sample rate: {sr}\")\n",
    "print(f\"Audio shape: {y.shape}\")\n",
    "print(f\"Duration: {len(y) / sr:.2f} seconds\")\n",
    "\n",
    "# Extract features\n",
    "features = extract_features(y, sr)\n",
    "print(f\"\\nExtracted features:\")\n",
    "for key, value in features.items():\n",
    "    if isinstance(value, np.ndarray):\n",
    "        print(f\"  {key}: shape {value.shape}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f495f16",
   "metadata": {},
   "source": [
    "## 4. Model Training\n",
    "\n",
    "For full training, use the command-line script:\n",
    "```bash\n",
    "python training/train.py --dataset ravdess --epochs 10 --batch-size 8\n",
    "```\n",
    "\n",
    "Here we'll demonstrate the training process step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b93f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model\n",
    "model_name = \"facebook/wav2vec2-base\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "\n",
    "# Define emotion labels\n",
    "emotion_labels = sorted(train_df['label'].unique())\n",
    "label2id = {label: idx for idx, label in enumerate(emotion_labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "print(f\"Emotion labels: {emotion_labels}\")\n",
    "print(f\"Label mapping: {label2id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeb015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a small batch for demonstration\n",
    "def prepare_batch(df, num_samples=4):\n",
    "    \"\"\"Prepare a small batch of data.\"\"\"\n",
    "    batch = df.sample(n=num_samples)\n",
    "    \n",
    "    audio_arrays = []\n",
    "    labels = []\n",
    "    \n",
    "    for _, row in batch.iterrows():\n",
    "        y, sr = librosa.load(row['file_path'], sr=16000)\n",
    "        audio_arrays.append(y)\n",
    "        labels.append(label2id[row['label']])\n",
    "    \n",
    "    # Process with Wav2Vec2 processor\n",
    "    inputs = processor(\n",
    "        audio_arrays,\n",
    "        sampling_rate=16000,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        max_length=16000 * 10,  # Max 10 seconds\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    inputs['labels'] = torch.tensor(labels)\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "# Test batch preparation\n",
    "batch = prepare_batch(train_df, num_samples=4)\n",
    "print(f\"Batch input_values shape: {batch['input_values'].shape}\")\n",
    "print(f\"Batch labels: {batch['labels']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02ed3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(emotion_labels),\n",
    "    label2id=label2id,\n",
    "    id2label=id2label\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "print(f\"Model loaded on {device}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f6d071",
   "metadata": {},
   "source": [
    "## 5. Inference Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02d3f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference on a sample\n",
    "model.eval()\n",
    "\n",
    "# Prepare single sample\n",
    "sample_path = test_df.iloc[0]['file_path']\n",
    "true_label = test_df.iloc[0]['label']\n",
    "\n",
    "y, sr = librosa.load(sample_path, sr=16000)\n",
    "inputs = processor(y, sampling_rate=16000, return_tensors='pt', padding=True)\n",
    "\n",
    "# Move to device\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)[0]\n",
    "    predicted_id = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "predicted_label = id2label[predicted_id]\n",
    "\n",
    "print(f\"True label: {true_label}\")\n",
    "print(f\"Predicted label: {predicted_label}\")\n",
    "print(f\"\\nProbabilities:\")\n",
    "for i, prob in enumerate(probs.cpu().numpy()):\n",
    "    print(f\"  {id2label[i]:12s}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca90d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Waveform\n",
    "librosa.display.waveshow(y, sr=sr, ax=ax1)\n",
    "ax1.set_title(f\"Audio Waveform\\nTrue: {true_label} | Predicted: {predicted_label}\")\n",
    "\n",
    "# Probabilities\n",
    "probs_np = probs.cpu().numpy()\n",
    "ax2.bar(emotion_labels, probs_np)\n",
    "ax2.set_title(\"Emotion Probabilities\")\n",
    "ax2.set_ylabel(\"Probability\")\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f968cef6",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Full Training**: Run the complete training script:\n",
    "   ```bash\n",
    "   python training/train.py --dataset ravdess --epochs 10 --batch-size 8\n",
    "   ```\n",
    "\n",
    "2. **Monitor Training**: Use Weights & Biases:\n",
    "   ```bash\n",
    "   python training/train.py --dataset ravdess --use-wandb\n",
    "   ```\n",
    "\n",
    "3. **Custom Dataset**: Prepare your own data:\n",
    "   ```bash\n",
    "   python training/prepare_custom.py\n",
    "   python training/train.py --dataset custom\n",
    "   ```\n",
    "\n",
    "4. **Deploy Model**: Update the backend to use your trained model:\n",
    "   - Edit `backend/app/models/emotion_inference.py`\n",
    "   - Change model path to your checkpoint\n",
    "   - Restart the backend server"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
